{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "data_labeled = np.load('/Users/catalinabartholomew/Documents/msdsMac/independentProjs/unet_cells/counting-cells-in-microscopy-images-2023/train_data.npz')\n",
    "#checks how well we did... use later\n",
    "test_images = np.load('/Users/catalinabartholomew/Documents/msdsMac/independentProjs/unet_cells/counting-cells-in-microscopy-images-2023/test_images.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_labeled = data_labeled['X'] #then split into x train and xval  #numpy array\n",
    "y_labeled = data_labeled['y'] #then split into ytrain and yval\n",
    "#use xtrain to predict ytrain.. use xlabeled to predict ylabeled\n",
    "\n",
    "#splitting the data into training and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = sk.model_selection.train_test_split(X_labeled, y_labeled, train_size=.8)\n",
    "\n",
    "#what is test images?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(X_train))\n",
    "x = X_train[i]\n",
    "y = y_train[i]\n",
    "print(x.shape)\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(y, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CellsDataset():\n",
    "  def __init__(self, X, y):\n",
    "    # X and y are numpy arrays\n",
    "    # X contains a bunch of images (Maybe 1600 images, for example)\n",
    "    # y contains corresponding target images\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "\n",
    "    x = self.X[i]/255.0  # x is the ith image in our dataset\n",
    "    y = self.y[i]\n",
    "    x = x.reshape((1, 128, 128))\n",
    "    y = y.reshape((1, 128, 128))\n",
    "\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CellsDatasetTest():\n",
    "  def __init__(self, X):\n",
    "    # X and y are numpy arrays\n",
    "    # X contains a bunch of images (Maybe 1600 images, for example)\n",
    "    # no labels in test data\n",
    "    self.X = X\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "\n",
    "    x = self.X[i]/255.0\n",
    "    x = x.reshape((1, 128, 128))\n",
    "\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class, dataloader objects\n",
    "dataset_train = CellsDataset(X_train, y_train)\n",
    "dataset_val = CellsDataset(X_val, y_val)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True) #why only x and not y? #grabs a batch of 16 x_train obs\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=16, shuffle=False)\n",
    "\n",
    "x_batch, y_batch = next(iter(dataloader_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetCNN(torch.nn.Module): #change this to Unet\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "                            # depth, num filters, 3x3 filter, 0 padding as to maintain size\n",
    "    self.conv1 = torch.nn.Conv2d(1, 64, kernel_size=3, padding='same') #does the input have a depth of 1 or 16?\n",
    "    self.conv2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
    "    self.conv2_1 = torch.nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
    "    self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, padding='same') #128 filters with depth of 64\n",
    "    self.conv4 = torch.nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
    "    self.conv4_1 = torch.nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
    "    self.conv5 = torch.nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
    "    self.conv6 = torch.nn.Conv2d(256, 256, kernel_size=3, padding='same')\n",
    "    self.conv6_1 = torch.nn.Conv2d(256, 256, kernel_size=3, padding='same')\n",
    "    self.conv7 = torch.nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
    "    self.conv8 = torch.nn.Conv2d(512, 512, kernel_size=3, padding='same')\n",
    "    self.conv8_1 = torch.nn.Conv2d(512, 512, kernel_size=3, padding='same')\n",
    "    self.conv9 = torch.nn.Conv2d(512, 1024, kernel_size=3, padding='same')\n",
    "    self.conv10 = torch.nn.Conv2d(1024, 1024, kernel_size=3, padding='same')\n",
    "    self.conv15 = torch.nn.Conv2d(64, 1, kernel_size=3, padding='same')\n",
    "#upconv:\n",
    "    self.conv11 = torch.nn.Conv2d(1024, 512, kernel_size=3, padding='same')\n",
    "    self.conv11_1 = torch.nn.Conv2d(1024, 512, kernel_size=3, padding='same')\n",
    "    self.conv12 = torch.nn.Conv2d(512, 256, kernel_size=3, padding='same')\n",
    "    self.conv12_1 = torch.nn.Conv2d(512, 256, kernel_size=3, padding='same')\n",
    "    self.conv13 = torch.nn.Conv2d(256, 128, kernel_size=3, padding='same')\n",
    "    self.conv13_1 = torch.nn.Conv2d(256, 128, kernel_size=3, padding='same')\n",
    "    self.conv14 = torch.nn.Conv2d(128, 64, kernel_size=3, padding='same')\n",
    "    self.conv14_1 = torch.nn.Conv2d(128, 64, kernel_size=3, padding='same')\n",
    "\n",
    "\n",
    "    #self.dense1 = torch.nn.Linear(64*14*14, 10)\n",
    "\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.upsample = torch.nn.Upsample(scale_factor=2) #upsampling\n",
    "    #directly use torch.cat , doesnt need to be in the constrictor ie.) x = torch.cat((x, x2), 0)\n",
    "\n",
    "    #unet needds forwards which contains all opertaions we perform\n",
    "    #upconv, copy and crop\n",
    "      #up conv has 2 setps: upsampling (doubles like reverse max pool)0and apply apply conv layer\n",
    "    #dims: input has 1x128x128\n",
    "    #conv 1, 2 uses 64\n",
    "\n",
    "##every upconv need to be concatenated!\n",
    "\n",
    "  def forward(self, x): #specified the exact calulation that our nn performs\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    x = self.conv1(x)\n",
    "    x = self.relu(x) #sets any negative enteries to 0 #64,128,128\n",
    "\n",
    "    x = self.conv2(x)\n",
    "    x1 = self.relu(x) #64,128,128\n",
    "\n",
    "    x = self.maxpool(x1) #after this step we have size 64x64x64\n",
    "    ####\n",
    "    x = self.conv3(x)\n",
    "    x = self.relu(x) #128,64,64\n",
    "\n",
    "    x = self.conv4(x)\n",
    "    x2 = self.relu(x) #128,64,64\n",
    "\n",
    "    x = self.maxpool(x2) #128x32x32\n",
    "    ###\n",
    "    x = self.conv5(x) #256,32,32\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.conv6(x)\n",
    "    x3 = self.relu(x) #256,32,32\n",
    "\n",
    "    x = self.maxpool(x3) #256x16x16\n",
    "    ###\n",
    "    x = self.conv7(x)#512,16,16\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.conv8(x)\n",
    "    x4 = self.relu(x)#512,16,16\n",
    "\n",
    "    x = self.maxpool(x4) #512x8x8\n",
    "    ###\n",
    "    x = self.conv9(x) #1024,8,8\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.conv10(x)\n",
    "    x = self.relu(x)#1024,8,8\n",
    "\n",
    "    x = self.upsample(x) #1024x16x16 #torch.nn.Upsample()\n",
    "    x = self.conv11(x)\n",
    "\n",
    "\n",
    "    xA = self.relu(x) #512,16,16\n",
    "    #relu???????? when upconv? ^?\n",
    "    #concat depthwise: xA + x4\n",
    "\n",
    "    ##concat!! #1024,16,16\n",
    "\n",
    "    x = torch.cat((xA,x4), dim = 1)#\n",
    "\n",
    "    #first step after the first concat:\n",
    "    x = self.conv11_1(x)\n",
    "    x = self.relu(x)#512,16,16\n",
    "\n",
    "\n",
    "    x = self.conv8_1(x)\n",
    "    x = self.relu(x) #512,16,16\n",
    "    ##upsample\n",
    "    x = self.upsample(x) #512x32x32\n",
    "    #upconv:\n",
    "    x = self.conv12(x)\n",
    "    xB = self.relu(x)#256,32, 32\n",
    "    #relu?^\n",
    "\n",
    "    #concat xb and x3 #512x32x32\n",
    "    x = torch.cat((xB,x3), dim = 1)\n",
    "\n",
    "    ###\n",
    "    x = self.conv12_1(x)\n",
    "    x = self.relu(x) #256,32,32\n",
    "\n",
    "    x = self.conv6_1(x)\n",
    "    x = self.relu(x) #256,32,32\n",
    "\n",
    "    #upsample:\n",
    "    x = self.upsample(x) #256x64x64\n",
    "    #upconv; star c\n",
    "    x = self.conv13(x)\n",
    "    xC = self.relu(x) #128,64,64\n",
    "\n",
    "    #concat xc and x2; should be 256x64x64\n",
    "    x = torch.cat((xC,x2), dim = 1)\n",
    "\n",
    "    # x = torch.cat((xc,x2), dim = 1)\n",
    "\n",
    "    x = self.conv13_1(x) #128x64x64\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.conv4_1(x)\n",
    "    x = self.relu(x)#128x64x64\n",
    "\n",
    "    #upsample:\n",
    "    x = self.upsample(x) #128x128x128\n",
    "    #upconv\n",
    "    x = self.conv14(x)\n",
    "    xD = self.relu(x) #64,128,128\n",
    "\n",
    "    #concat: xd and x1 #128,128,128\n",
    "    x = torch.cat((xD,x1), dim = 1)\n",
    "\n",
    "\n",
    "    x = self.conv14_1(x)\n",
    "    x = self.relu(x) #64x128x128\n",
    "\n",
    "    x = self.conv2_1(x)\n",
    "    x = self.relu(x)#64x128x128\n",
    "\n",
    "    x = self.conv15(x) #1, 128, 128\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main???\n",
    "\n",
    "# softmax = torch.nn.Softmax() # should be sigmoid\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "#stochasic gradient descent\n",
    "model = UnetCNN() #change to uNet\n",
    "device = torch.device('cuda') #gpu noises\n",
    "#device = torch.device('cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.0003)\n",
    "#loss_fun = torch.nn.CrossEntropyLoss() #is this what i want?\n",
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "#training loop!\n",
    "num_epochs = 65\n",
    "ace_vals_train = []\n",
    "ace_vals_val = []\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "  print(f'ep is: {ep}')\n",
    "  for x_batch, y_batch in dataloader_train:\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    outputs = model(x_batch) #is outputs a 16x1 col vector describing 16 images? i need to consider one at a time and round?\n",
    "    loss = loss_fun(outputs, y_batch) #bug here :Target 1 is out of bounds.\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  #with an\n",
    "  with torch.no_grad():\n",
    "    ace = 0\n",
    "    #4 send ola pync\n",
    "    for x_batch, y_batch in dataloader_train:\n",
    "      x_batch = x_batch.to(device)\n",
    "      y_batch = y_batch.to(device)\n",
    "      outputs = model(x_batch)\n",
    "      loss = loss_fun(outputs, y_batch)\n",
    "\n",
    "      ace = ace + loss * len(y_batch)\n",
    "\n",
    "    #2acc, 3ace\n",
    "\n",
    "    ace = ace / len(dataset_train)\n",
    "    ace = ace.item()\n",
    "    ace_vals_train.append(ace)\n",
    "    #print ace and acc_train\n",
    "    print(f'Average cross entropy (training): {ace}')\n",
    "\n",
    "\n",
    "    #validation loop\n",
    "    ace = 0\n",
    "    #4send ola pync\n",
    "    for x_batch, y_batch in dataloader_val:\n",
    "      x_batch = x_batch.to(device)\n",
    "      y_batch = y_batch.to(device)\n",
    "      outputs = model(x_batch)\n",
    "      loss = loss_fun(outputs, y_batch)\n",
    "      ace = ace + loss * len(y_batch)\n",
    "\n",
    "    #2acc 3ace pp\n",
    "\n",
    "    ace = ace / len(dataset_val)\n",
    "    ace = ace.item()\n",
    "    ace_vals_val.append(ace)\n",
    "\n",
    "    print(f'Average cross entropy (validation): {ace}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ace_vals_train, label='Training')\n",
    "plt.plot(ace_vals_val, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Cross Entropy')\n",
    "plt.title('Training and Validation Objective Function Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "with torch.no_grad():\n",
    "  x_batch, y_batch = next(iter(dataloader_val))\n",
    "  x_batch = x_batch.to(device)\n",
    "  y_pred = sigmoid(model(x_batch))\n",
    "  y_pred = y_pred.cpu().numpy()\n",
    "  y_batch = y_batch.numpy()\n",
    "  i = np.random.randint(len(x_batch))\n",
    "  img = x_batch[i].cpu()[0]\n",
    "  plt.figure(figsize=(2,2))\n",
    "  plt.imshow(img, cmap='gray')\n",
    "\n",
    "  plt.figure(figsize=(2,2))\n",
    "  plt.imshow(y_batch[i][0])\n",
    "  plt.title('ground truth labels')\n",
    "\n",
    "  plt.figure(figsize=(2,2))\n",
    "  plt.imshow(y_pred[i][0], cmap='gray')\n",
    "  plt.title('predicted probabilities')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#after training, when considering test data, we round the output of our trained model!\n",
    "\n",
    "#what is the name of our trained model? // how to access?.. outputs!\n",
    "\n",
    "dataset_test = CellsDatasetTest(test_images['X'])\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=16, shuffle=False)\n",
    "\n",
    "x_batch = next(iter(dataloader_test))#KeyError: '0 is not a file in the archive'\n",
    "\n",
    "cell_counts65 = []\n",
    "with torch.no_grad():\n",
    "  for x_batch in dataloader_test:\n",
    "        x_batch = x_batch.to(device)\n",
    "        #y_batch = y_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        probabilities = sigmoid(outputs)\n",
    "        labels = torch.round(probabilities) #rounded output\n",
    "\n",
    "        for i in range(len(x_batch)):\n",
    "          labels_i = labels[i].cpu().numpy().astype('uint8')[0]\n",
    "\n",
    "          info = cv2.connectedComponents(labels_i)\n",
    "          count = info[0]\n",
    "          cell_counts65.append(count)\n",
    "\n",
    "# calculate accuracy to evakuate model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame({'index': range(len(cell_counts65)), 'count': cell_counts65})\n",
    "df_submit.to_csv('cell_counts65.csv', index=False)\n",
    "print(df_submit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
